services:
    # 1. Base de Datos: El cerebro persistente de Airflow
    postgres:
        image: postgres:15-alpine
        container_name: helios-postgres
        restart: always
        environment:
            - POSTGRES_USER=${POSTGRES_USER}
            - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
            - POSTGRES_DB=${POSTGRES_DB}
        volumes:
            - postgres_data:/var/lib/postgresql/data
        ports:
            - "${POSTGRES_PORT}:5432"
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
            interval: 5s
            timeout: 5s
            retries: 5
        networks:
            - helios_net

    # 2. Ingesta de Datos: El sistema nervioso de nuestro flujo
    zookeeper:
        image: confluentinc/cp-zookeeper:7.3.0
        container_name: helios-zookeeper
        networks:
            - helios_net
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000

    kafka:
        image: confluentinc/cp-kafka:7.3.0
        container_name: helios-kafka
        networks:
            - helios_net
        depends_on:
            - zookeeper
        ports:
            - "${KAFKA_PORT}:${KAFKA_PORT}"
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:${KAFKA_PORT}
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
            KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
        healthcheck:
            test: ["CMD", "cub", "kafka-ready", "-b", "localhost:9092", "1", "1"]
            interval: 10s
            timeout: 5s
            retries: 5
    
    kafka-setup:
        image: confluentinc/cp-kafka:7.3.0
        container_name: helios-kafka-setup
        depends_on:
            kafka:
                condition: service_healthy
        networks:
            - helios_net
        command: >
            bash -c "
                echo 'Esperando a que Kafka esté listo...' &&
                cub kafka-ready -b kafka:9092 1 30 &&
                echo 'Kafka listo. Creando topic...' &&
                kafka-topics --create --if-not-exists --topic solar-power-data --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 &&
                echo 'Topic creado exitosamente.'
            "

    # 3. Orquestador: El director de la sinfonía de datos
    # Esta configuración se basa en la oficial de Airflow
    # https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml
    airflow-init:
        image: apache/airflow:2.7.1
        container_name: helios-airflow-init
        depends_on:
            postgres:
                condition: service_healthy
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
            - AIRFLOW__CORE__LOAD_EXAMPLES=false
        command: bash -c "airflow db init && airflow users create --role Admin --username admin --password admin --email admin@example.com --firstname Admin --lastname User"
        networks:
            - helios_net

    airflow-webserver:
        image: apache/airflow:2.7.1
        build: 
            context: .
            dockerfile: Dockerfile
        container_name: helios-airflow-webserver
        restart: always
        depends_on:
            airflow-init:
                condition: service_completed_successfully
        ports:
            - "8080:8080"
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
            - /var/run/docker.sock:/var/run/docker.sock
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
            - KAFKA_PORT=${KAFKA_PORT}
            - DATA_FILE_PATH=${DATA_FILE_PATH}
        command: airflow webserver
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 30s
            timeout: 10s
            retries: 5
        networks:
            - helios_net

    airflow-scheduler:
        image: apache/airflow:2.7.1
        build: 
            context: .
            dockerfile: Dockerfile
        container_name: helios-airflow-scheduler
        restart: always
        depends_on:
            airflow-init:
                condition: service_completed_successfully
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
            - /var/run/docker.sock:/var/run/docker.sock
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
            - KAFKA_PORT=${KAFKA_PORT}
            - DATA_FILE_PATH=${DATA_FILE_PATH}
        command: scheduler
        networks:
            - helios_net

    # 4. Productor y Consumidor: Los actores principales en la escena de datos
    producer:
        build:
            context: ./producer
        container_name: helios-producer
        environment:
            - KAFKA_BROKER=kafka:${KAFKA_PORT}
            - DATA_FILE_PATH=${DATA_FILE_PATH}
        restart: on-failure
        depends_on:
            kafka-setup:
                condition: service_completed_successfully
        networks:
            - helios_net

    consumer:
        build:
            context: ./consumer
        container_name: helios-consumer
        restart: always
        depends_on:
            kafka:
                condition: service_healthy
            postgres:
                condition: service_healthy
        networks:
            - helios_net
        environment:
            - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
            - KAFKA_BROKER=kafka:${KAFKA_PORT}
            - POSTGRES_USER=${POSTGRES_USER}
            - KAFKA_TOPIC=solar-power-data
            - POSTGRES_DB=${POSTGRES_DB}
            - POSTGRES_HOST=postgres

# Red y Volúmenes para desacoplar y persistir
networks:
    helios_net:
        driver: bridge

volumes:
    postgres_data:
        driver: local
