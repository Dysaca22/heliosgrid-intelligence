services:
    # 1. Base de Datos: El cerebro persistente de Airflow
    postgres:
        image: postgres:15-alpine
        container_name: helios-postgres
        restart: always
        environment:
            - POSTGRES_USER=${POSTGRES_USER}
            - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
            - POSTGRES_DB=${POSTGRES_DB}
        volumes:
            - postgres_data:/var/lib/postgresql/data
        ports:
            - "${POSTGRES_PORT}:5432"
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
            interval: 5s
            timeout: 5s
            retries: 5
        networks:
            - helios_net

    # 2. Ingesta de Datos: El sistema nervioso de nuestro flujo
    zookeeper:
        image: confluentinc/cp-zookeeper:7.3.0
        container_name: helios-zookeeper
        networks:
            - helios_net
        environment:
            ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_PORT}
            ZOOKEEPER_TICK_TIME: 2000

    kafka:
        image: confluentinc/cp-kafka:7.3.0
        container_name: helios-kafka
        networks:
            - helios_net
        depends_on:
            - zookeeper
        ports:
            - "${KAFKA_PORT}:${KAFKA_PORT}"
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: "zookeeper:${ZOOKEEPER_PORT}"
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:${KAFKA_PORT},PLAINTEXT_INTERNAL://kafka:29092
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
            KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

    # 3. Orquestador: El director de la sinfonía de datos
    # Esta configuración se basa en la oficial de Airflow
    # https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml
    airflow-init:
        image: apache/airflow:2.7.1
        container_name: helios-airflow-init
        depends_on:
            - postgres
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/${POSTGRES_DB}
            - AIRFLOW__CORE__LOAD_EXAMPLES=false
        command: bash -c "airflow db init && airflow users create --role Admin --username admin --password admin --email admin@example.com --firstname Admin --lastname User"
        networks:
            - helios_net

    airflow-webserver:
        image: apache/airflow:2.7.1
        container_name: helios-airflow-webserver
        restart: always
        depends_on:
            airflow-init:
                condition: service_completed_successfully
        ports:
            - "8080:8080"
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/${POSTGRES_DB}
        command: airflow webserver
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 30s
            timeout: 10s
            retries: 5
        networks:
            - helios_net

    # Añadiremos airflow-scheduler y otros componentes cuando sea necesario para mantener la simplicidad inicial.

# Red y Volúmenes para desacoplar y persistir
networks:
    helios_net:
        driver: bridge

volumes:
    postgres_data:
        driver: local
